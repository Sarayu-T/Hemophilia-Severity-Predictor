{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cDNA</th>\n",
       "      <th>Amino acid (HGVS)</th>\n",
       "      <th>Amino acid (Legacy)</th>\n",
       "      <th>pos</th>\n",
       "      <th>Domain_A2</th>\n",
       "      <th>Domain_A3</th>\n",
       "      <th>Domain_B</th>\n",
       "      <th>Domain_C1</th>\n",
       "      <th>Domain_C2</th>\n",
       "      <th>Domain_Signal Peptide</th>\n",
       "      <th>...</th>\n",
       "      <th>Locationingene_7</th>\n",
       "      <th>Locationingene_8</th>\n",
       "      <th>Locationingene_9</th>\n",
       "      <th>nitroBaseBef_C</th>\n",
       "      <th>nitroBaseBef_G</th>\n",
       "      <th>nitroBaseBef_T</th>\n",
       "      <th>nitroBaseAft_C</th>\n",
       "      <th>nitroBaseAft_G</th>\n",
       "      <th>nitroBaseAft_T</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.411869</td>\n",
       "      <td>-1.412448</td>\n",
       "      <td>-1.412429</td>\n",
       "      <td>2.224128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.499705</td>\n",
       "      <td>-1.499467</td>\n",
       "      <td>-1.500672</td>\n",
       "      <td>-0.958553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.499705</td>\n",
       "      <td>-1.499467</td>\n",
       "      <td>-1.500672</td>\n",
       "      <td>-0.958553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.499705</td>\n",
       "      <td>-1.499467</td>\n",
       "      <td>-1.500672</td>\n",
       "      <td>-0.958553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.499705</td>\n",
       "      <td>-1.499467</td>\n",
       "      <td>-1.500672</td>\n",
       "      <td>-0.958553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 482 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cDNA  Amino acid (HGVS)  Amino acid (Legacy)       pos  Domain_A2  \\\n",
       "0 -1.411869          -1.412448            -1.412429  2.224128        0.0   \n",
       "1 -1.499705          -1.499467            -1.500672 -0.958553        0.0   \n",
       "2 -1.499705          -1.499467            -1.500672 -0.958553        0.0   \n",
       "3 -1.499705          -1.499467            -1.500672 -0.958553        0.0   \n",
       "4 -1.499705          -1.499467            -1.500672 -0.958553        0.0   \n",
       "\n",
       "   Domain_A3  Domain_B  Domain_C1  Domain_C2  Domain_Signal Peptide  ...  \\\n",
       "0        0.0       0.0        0.0        0.0                    0.0  ...   \n",
       "1        0.0       0.0        0.0        0.0                    1.0  ...   \n",
       "2        0.0       0.0        0.0        0.0                    1.0  ...   \n",
       "3        0.0       0.0        0.0        0.0                    1.0  ...   \n",
       "4        0.0       0.0        0.0        0.0                    1.0  ...   \n",
       "\n",
       "   Locationingene_7  Locationingene_8  Locationingene_9  nitroBaseBef_C  \\\n",
       "0               0.0               0.0               0.0             0.0   \n",
       "1               0.0               0.0               0.0             0.0   \n",
       "2               0.0               0.0               0.0             0.0   \n",
       "3               0.0               0.0               0.0             0.0   \n",
       "4               0.0               0.0               0.0             0.0   \n",
       "\n",
       "   nitroBaseBef_G  nitroBaseBef_T  nitroBaseAft_C  nitroBaseAft_G  \\\n",
       "0             0.0             0.0             1.0             0.0   \n",
       "1             0.0             0.0             0.0             1.0   \n",
       "2             0.0             0.0             0.0             1.0   \n",
       "3             0.0             0.0             0.0             1.0   \n",
       "4             0.0             0.0             0.0             1.0   \n",
       "\n",
       "   nitroBaseAft_T  Severity  \n",
       "0             0.0         0  \n",
       "1             0.0         2  \n",
       "2             0.0         2  \n",
       "3             0.0         2  \n",
       "4             0.0         2  \n",
       "\n",
       "[5 rows x 482 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 2s 4ms/step - loss: 0.9397 - accuracy: 0.5659 - val_loss: 0.8311 - val_accuracy: 0.6175\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.7588 - accuracy: 0.6533 - val_loss: 0.8064 - val_accuracy: 0.6279\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.6905 - val_loss: 0.7956 - val_accuracy: 0.6528\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.7067 - val_loss: 0.7861 - val_accuracy: 0.6678\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.7258 - val_loss: 0.7696 - val_accuracy: 0.6661\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.7376 - val_loss: 0.7764 - val_accuracy: 0.6817\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5836 - accuracy: 0.7409 - val_loss: 0.7775 - val_accuracy: 0.6863\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5627 - accuracy: 0.7541 - val_loss: 0.7727 - val_accuracy: 0.6765\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5487 - accuracy: 0.7651 - val_loss: 0.7664 - val_accuracy: 0.6806\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5368 - accuracy: 0.7691 - val_loss: 0.7611 - val_accuracy: 0.6939\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7611 - accuracy: 0.6939\n",
      "Fold 1 - Loss: 0.7611333131790161, Accuracy: 0.6938657164573669\n",
      "Processing fold 2\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 2s 4ms/step - loss: 0.9341 - accuracy: 0.5664 - val_loss: 0.8185 - val_accuracy: 0.6265\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7697 - accuracy: 0.6432 - val_loss: 0.7749 - val_accuracy: 0.6410\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7137 - accuracy: 0.6800 - val_loss: 0.7463 - val_accuracy: 0.6740\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.7029 - val_loss: 0.7266 - val_accuracy: 0.7041\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.7225 - val_loss: 0.7210 - val_accuracy: 0.7041\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.7370 - val_loss: 0.7165 - val_accuracy: 0.7001\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5909 - accuracy: 0.7482 - val_loss: 0.6962 - val_accuracy: 0.7197\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5716 - accuracy: 0.7521 - val_loss: 0.6877 - val_accuracy: 0.7180\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5561 - accuracy: 0.7615 - val_loss: 0.7068 - val_accuracy: 0.7093\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5440 - accuracy: 0.7632 - val_loss: 0.6927 - val_accuracy: 0.7221\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.7221\n",
      "Fold 2 - Loss: 0.6927215456962585, Accuracy: 0.7220613956451416\n",
      "Processing fold 3\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 3s 5ms/step - loss: 0.9384 - accuracy: 0.5627 - val_loss: 0.8249 - val_accuracy: 0.6039\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7558 - accuracy: 0.6509 - val_loss: 0.7841 - val_accuracy: 0.6462\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6976 - accuracy: 0.6895 - val_loss: 0.7704 - val_accuracy: 0.6786\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.7123 - val_loss: 0.7593 - val_accuracy: 0.6809\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.7292 - val_loss: 0.7408 - val_accuracy: 0.6960\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6046 - accuracy: 0.7393 - val_loss: 0.7324 - val_accuracy: 0.7006\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.7509 - val_loss: 0.7401 - val_accuracy: 0.6694\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5667 - accuracy: 0.7538 - val_loss: 0.7229 - val_accuracy: 0.7116\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5510 - accuracy: 0.7593 - val_loss: 0.7178 - val_accuracy: 0.7105\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5407 - accuracy: 0.7657 - val_loss: 0.7351 - val_accuracy: 0.7145\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7351 - accuracy: 0.7145\n",
      "Fold 3 - Loss: 0.7350667715072632, Accuracy: 0.7145338654518127\n",
      "Processing fold 4\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 2s 5ms/step - loss: 0.9480 - accuracy: 0.5542 - val_loss: 0.8544 - val_accuracy: 0.6028\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7501 - accuracy: 0.6628 - val_loss: 0.8064 - val_accuracy: 0.6543\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6947 - accuracy: 0.6895 - val_loss: 0.8020 - val_accuracy: 0.6589\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.7099 - val_loss: 0.7832 - val_accuracy: 0.6676\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.7267 - val_loss: 0.7861 - val_accuracy: 0.6763\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5974 - accuracy: 0.7437 - val_loss: 0.7785 - val_accuracy: 0.6879\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.7525 - val_loss: 0.7818 - val_accuracy: 0.6914\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5580 - accuracy: 0.7589 - val_loss: 0.7717 - val_accuracy: 0.6943\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7694 - val_loss: 0.7835 - val_accuracy: 0.6873\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5340 - accuracy: 0.7660 - val_loss: 0.7762 - val_accuracy: 0.7012\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7762 - accuracy: 0.7012\n",
      "Fold 4 - Loss: 0.7762477397918701, Accuracy: 0.7012159824371338\n",
      "Processing fold 5\n",
      "Epoch 1/10\n",
      "216/216 [==============================] - 2s 5ms/step - loss: 0.9505 - accuracy: 0.5474 - val_loss: 0.8447 - val_accuracy: 0.6086\n",
      "Epoch 2/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.7663 - accuracy: 0.6409 - val_loss: 0.7966 - val_accuracy: 0.6497\n",
      "Epoch 3/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.7083 - accuracy: 0.6751 - val_loss: 0.7815 - val_accuracy: 0.6699\n",
      "Epoch 4/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.7039 - val_loss: 0.7746 - val_accuracy: 0.6856\n",
      "Epoch 5/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6378 - accuracy: 0.7233 - val_loss: 0.7619 - val_accuracy: 0.6948\n",
      "Epoch 6/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.7351 - val_loss: 0.7613 - val_accuracy: 0.6891\n",
      "Epoch 7/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5881 - accuracy: 0.7476 - val_loss: 0.7606 - val_accuracy: 0.6948\n",
      "Epoch 8/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5691 - accuracy: 0.7506 - val_loss: 0.7529 - val_accuracy: 0.7070\n",
      "Epoch 9/10\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.5531 - accuracy: 0.7613 - val_loss: 0.7601 - val_accuracy: 0.7064\n",
      "Epoch 10/10\n",
      "216/216 [==============================] - 1s 3ms/step - loss: 0.5412 - accuracy: 0.7639 - val_loss: 0.7605 - val_accuracy: 0.7169\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7605 - accuracy: 0.7169\n",
      "Fold 5 - Loss: 0.7605297565460205, Accuracy: 0.7168500423431396\n",
      "Average Accuracy across all folds: 0.709705400466919\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Adjust n_splits as needed\n",
    "\n",
    "fold_accuracies = []  # List to store accuracy of each fold\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=50, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), return_sequences=False))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming Severity has 3 classes: 0, 1, 2\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, validation_data=(X_val_rnn, y_val), verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 5s 10ms/step - loss: 1.0225 - accuracy: 0.5232 - val_loss: 0.8384 - val_accuracy: 0.5961\n",
      "Epoch 2/20\n",
      " 29/216 [===>..........................] - ETA: 0s - loss: 0.8753 - accuracy: 0.5991"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saray\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8919 - accuracy: 0.5869 - val_loss: 0.8267 - val_accuracy: 0.6123\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8774 - accuracy: 0.6022 - val_loss: 0.8693 - val_accuracy: 0.6042\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8584 - accuracy: 0.6129 - val_loss: 0.8392 - val_accuracy: 0.6267\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8523 - accuracy: 0.6129 - val_loss: 0.8111 - val_accuracy: 0.6372\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8452 - accuracy: 0.6252 - val_loss: 0.8397 - val_accuracy: 0.6128\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8559 - accuracy: 0.6175 - val_loss: 0.8215 - val_accuracy: 0.6366\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8407 - accuracy: 0.6229 - val_loss: 0.8259 - val_accuracy: 0.6372\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.8259 - accuracy: 0.6372\n",
      "Fold 1 - Loss: 0.8258816599845886, Accuracy: 0.6371527910232544\n",
      "Processing fold 2\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 7ms/step - loss: 1.0314 - accuracy: 0.5166 - val_loss: 0.8462 - val_accuracy: 0.6063\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.9127 - accuracy: 0.5798 - val_loss: 0.8085 - val_accuracy: 0.6375\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8797 - accuracy: 0.5957 - val_loss: 0.8428 - val_accuracy: 0.6236\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8763 - accuracy: 0.6028 - val_loss: 0.8105 - val_accuracy: 0.6312\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8666 - accuracy: 0.6054 - val_loss: 0.7746 - val_accuracy: 0.6497\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8577 - accuracy: 0.6104 - val_loss: 0.8207 - val_accuracy: 0.6468\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.8526 - accuracy: 0.6230 - val_loss: 0.8037 - val_accuracy: 0.6103\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 2s 7ms/step - loss: 0.8573 - accuracy: 0.6154 - val_loss: 0.7898 - val_accuracy: 0.6497\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7898 - accuracy: 0.6497\n",
      "Fold 2 - Loss: 0.7897911071777344, Accuracy: 0.6496815085411072\n",
      "Processing fold 3\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 7ms/step - loss: 1.0217 - accuracy: 0.5274 - val_loss: 0.8658 - val_accuracy: 0.6034\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8955 - accuracy: 0.5939 - val_loss: 0.8355 - val_accuracy: 0.6126\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8779 - accuracy: 0.6059 - val_loss: 0.8408 - val_accuracy: 0.6474\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8695 - accuracy: 0.6056 - val_loss: 0.8632 - val_accuracy: 0.6323\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8621 - accuracy: 0.6192 - val_loss: 0.8368 - val_accuracy: 0.6340\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.8368 - accuracy: 0.6340\n",
      "Fold 3 - Loss: 0.8368388414382935, Accuracy: 0.6340475082397461\n",
      "Processing fold 4\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 8ms/step - loss: 1.0133 - accuracy: 0.5310 - val_loss: 0.8777 - val_accuracy: 0.5947\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8943 - accuracy: 0.5860 - val_loss: 0.8221 - val_accuracy: 0.6300\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8739 - accuracy: 0.5991 - val_loss: 0.8210 - val_accuracy: 0.6445\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8664 - accuracy: 0.6030 - val_loss: 0.8339 - val_accuracy: 0.6375\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8632 - accuracy: 0.6125 - val_loss: 0.8470 - val_accuracy: 0.6225\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8550 - accuracy: 0.6147 - val_loss: 0.8719 - val_accuracy: 0.6364\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.8719 - accuracy: 0.6364\n",
      "Fold 4 - Loss: 0.8718913197517395, Accuracy: 0.6363636255264282\n",
      "Processing fold 5\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 8ms/step - loss: 1.0142 - accuracy: 0.5211 - val_loss: 0.8436 - val_accuracy: 0.6103\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.9010 - accuracy: 0.5800 - val_loss: 0.8365 - val_accuracy: 0.6173\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8728 - accuracy: 0.6049 - val_loss: 0.8663 - val_accuracy: 0.6393\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8616 - accuracy: 0.6154 - val_loss: 0.8492 - val_accuracy: 0.6317\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8558 - accuracy: 0.6157 - val_loss: 0.8101 - val_accuracy: 0.6404\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8404 - accuracy: 0.6224 - val_loss: 0.8399 - val_accuracy: 0.6375\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8488 - accuracy: 0.6208 - val_loss: 0.8204 - val_accuracy: 0.6416\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.8423 - accuracy: 0.6167 - val_loss: 0.8225 - val_accuracy: 0.6387\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.8225 - accuracy: 0.6387\n",
      "Fold 5 - Loss: 0.8225484490394592, Accuracy: 0.6386798024177551\n",
      "Average Accuracy across all folds: 0.6391850471496582\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Adjust n_splits as needed\n",
    "\n",
    "fold_accuracies = []  # List to store accuracy of each fold\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SimpleRNN(units=100, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming Severity has 3 classes: 0, 1, 2\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.01)  # You can experiment with different learning rates\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model_checkpoint = ModelCheckpoint(f'best_model_fold_{fold}.h5', save_best_only=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_rnn, y_train, epochs=20, batch_size=32, validation_data=(X_val_rnn, y_val),\n",
    "              callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "216/216 [==============================] - 3s 5ms/step - loss: 1.1016 - accuracy: 0.4770 - val_loss: 1.2466 - val_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      " 48/216 [=====>........................] - ETA: 0s - loss: 0.9380 - accuracy: 0.5560"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saray\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 1s 5ms/step - loss: 0.9047 - accuracy: 0.5735 - val_loss: 1.2238 - val_accuracy: 0.2755\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8472 - accuracy: 0.6100 - val_loss: 1.1703 - val_accuracy: 0.3351\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8155 - accuracy: 0.6287 - val_loss: 1.1352 - val_accuracy: 0.4659\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7855 - accuracy: 0.6401 - val_loss: 1.1371 - val_accuracy: 0.3692\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7679 - accuracy: 0.6497 - val_loss: 1.1339 - val_accuracy: 0.3767\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7550 - accuracy: 0.6579 - val_loss: 1.0778 - val_accuracy: 0.4161\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7374 - accuracy: 0.6682 - val_loss: 1.0734 - val_accuracy: 0.5307\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7273 - accuracy: 0.6714 - val_loss: 1.1326 - val_accuracy: 0.4005\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7269 - accuracy: 0.6675 - val_loss: 1.0744 - val_accuracy: 0.5237\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7116 - accuracy: 0.6799 - val_loss: 1.0302 - val_accuracy: 0.5284\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7080 - accuracy: 0.6860 - val_loss: 1.0590 - val_accuracy: 0.4340\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7001 - accuracy: 0.6892 - val_loss: 1.0236 - val_accuracy: 0.5706\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6969 - accuracy: 0.6938 - val_loss: 1.0663 - val_accuracy: 0.4479\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6908 - accuracy: 0.6911 - val_loss: 1.0400 - val_accuracy: 0.5596\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6908 - accuracy: 0.6918 - val_loss: 1.0133 - val_accuracy: 0.5804\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.6979 - val_loss: 1.0242 - val_accuracy: 0.4653\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.7053 - val_loss: 1.0108 - val_accuracy: 0.5694\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6732 - accuracy: 0.7037 - val_loss: 1.0402 - val_accuracy: 0.4740\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.7077 - val_loss: 1.0150 - val_accuracy: 0.5324\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 1.0150 - accuracy: 0.5324\n",
      "Loss: 1.0149518251419067, Accuracy: 0.5324074029922485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X_scaled) * split_ratio)\n",
    "\n",
    "X_train = X_scaled[:split_index]\n",
    "X_val = X_scaled[split_index:]\n",
    "y_train = y_one_hot[:split_index]\n",
    "y_val = y_one_hot[split_index:]\n",
    "\n",
    "# Reshape for RNN\n",
    "X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(SimpleRNN(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming Severity has 3 classes: 0, 1, 2\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)  # You can experiment with different learning rates\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_rnn, y_train, epochs=20, batch_size=32, validation_data=(X_val_rnn, y_val),\n",
    "                    callbacks=[early_stopping, model_checkpoint], verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Clear the model from memory\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 7ms/step - loss: 1.1734 - accuracy: 0.4748 - val_loss: 0.8751 - val_accuracy: 0.5718 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.9666 - accuracy: 0.5492 - val_loss: 0.8304 - val_accuracy: 0.6007 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.8744 - accuracy: 0.5932 - val_loss: 0.8204 - val_accuracy: 0.6065 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.8349 - accuracy: 0.6206 - val_loss: 0.8054 - val_accuracy: 0.6175 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7967 - accuracy: 0.6401 - val_loss: 0.7973 - val_accuracy: 0.6337 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7817 - accuracy: 0.6510 - val_loss: 0.7882 - val_accuracy: 0.6441 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7674 - accuracy: 0.6527 - val_loss: 0.7863 - val_accuracy: 0.6522 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7582 - accuracy: 0.6566 - val_loss: 0.7840 - val_accuracy: 0.6499 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7418 - accuracy: 0.6669 - val_loss: 0.7755 - val_accuracy: 0.6603 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7316 - accuracy: 0.6702 - val_loss: 0.7736 - val_accuracy: 0.6644 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.7199 - accuracy: 0.6801 - val_loss: 0.7725 - val_accuracy: 0.6632 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7139 - accuracy: 0.6824 - val_loss: 0.7593 - val_accuracy: 0.6626 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7051 - accuracy: 0.6891 - val_loss: 0.7543 - val_accuracy: 0.6811 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7047 - accuracy: 0.6911 - val_loss: 0.7588 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.7012 - val_loss: 0.7506 - val_accuracy: 0.6852 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6874 - accuracy: 0.6948 - val_loss: 0.7499 - val_accuracy: 0.6771 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6796 - accuracy: 0.7027 - val_loss: 0.7480 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.7057 - val_loss: 0.7501 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6727 - accuracy: 0.7035 - val_loss: 0.7377 - val_accuracy: 0.6863 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.7166 - val_loss: 0.7410 - val_accuracy: 0.6939 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7410 - accuracy: 0.6939\n",
      "Fold 1 - Loss: 0.7410372495651245, Accuracy: 0.6938657164573669\n",
      "Processing fold 2\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 8ms/step - loss: 1.1597 - accuracy: 0.4771 - val_loss: 0.8377 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.9613 - accuracy: 0.5528 - val_loss: 0.8071 - val_accuracy: 0.6277 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8960 - accuracy: 0.5824 - val_loss: 0.7906 - val_accuracy: 0.6433 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8448 - accuracy: 0.6149 - val_loss: 0.7794 - val_accuracy: 0.6387 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8103 - accuracy: 0.6322 - val_loss: 0.7768 - val_accuracy: 0.6549 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7847 - accuracy: 0.6422 - val_loss: 0.7665 - val_accuracy: 0.6549 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7719 - accuracy: 0.6520 - val_loss: 0.7583 - val_accuracy: 0.6630 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7641 - accuracy: 0.6567 - val_loss: 0.7521 - val_accuracy: 0.6665 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7509 - accuracy: 0.6607 - val_loss: 0.7500 - val_accuracy: 0.6728 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7496 - accuracy: 0.6661 - val_loss: 0.7335 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7369 - accuracy: 0.6730 - val_loss: 0.7341 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7232 - accuracy: 0.6839 - val_loss: 0.7278 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7177 - accuracy: 0.6852 - val_loss: 0.7244 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7128 - accuracy: 0.6837 - val_loss: 0.7224 - val_accuracy: 0.6844 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7073 - accuracy: 0.6887 - val_loss: 0.7187 - val_accuracy: 0.6925 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7050 - accuracy: 0.6905 - val_loss: 0.7150 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6912 - accuracy: 0.6971 - val_loss: 0.7122 - val_accuracy: 0.7018 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6885 - accuracy: 0.7047 - val_loss: 0.7061 - val_accuracy: 0.7018 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6781 - accuracy: 0.7039 - val_loss: 0.7015 - val_accuracy: 0.7053 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6783 - accuracy: 0.7057 - val_loss: 0.6958 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6958 - accuracy: 0.7035\n",
      "Fold 2 - Loss: 0.695758581161499, Accuracy: 0.7035321593284607\n",
      "Processing fold 3\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 7ms/step - loss: 1.1008 - accuracy: 0.4826 - val_loss: 0.8773 - val_accuracy: 0.5646 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.9408 - accuracy: 0.5575 - val_loss: 0.8369 - val_accuracy: 0.5987 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8569 - accuracy: 0.6012 - val_loss: 0.8090 - val_accuracy: 0.6242 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8222 - accuracy: 0.6269 - val_loss: 0.7967 - val_accuracy: 0.6346 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7884 - accuracy: 0.6455 - val_loss: 0.7966 - val_accuracy: 0.6271 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7764 - accuracy: 0.6481 - val_loss: 0.7914 - val_accuracy: 0.6427 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7638 - accuracy: 0.6560 - val_loss: 0.7793 - val_accuracy: 0.6456 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7489 - accuracy: 0.6603 - val_loss: 0.7742 - val_accuracy: 0.6572 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7363 - accuracy: 0.6699 - val_loss: 0.7700 - val_accuracy: 0.6630 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7326 - accuracy: 0.6774 - val_loss: 0.7560 - val_accuracy: 0.6723 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7208 - accuracy: 0.6764 - val_loss: 0.7552 - val_accuracy: 0.6694 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7163 - accuracy: 0.6824 - val_loss: 0.7518 - val_accuracy: 0.6688 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7000 - accuracy: 0.6936 - val_loss: 0.7475 - val_accuracy: 0.6792 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7060 - accuracy: 0.6921 - val_loss: 0.7415 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6910 - accuracy: 0.6950 - val_loss: 0.7377 - val_accuracy: 0.6838 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6851 - accuracy: 0.7040 - val_loss: 0.7290 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.7040 - val_loss: 0.7294 - val_accuracy: 0.6896 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6699 - accuracy: 0.7099 - val_loss: 0.7234 - val_accuracy: 0.6983 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.6681 - accuracy: 0.7072 - val_loss: 0.7156 - val_accuracy: 0.7001 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6656 - accuracy: 0.7065 - val_loss: 0.7147 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.7147 - accuracy: 0.6931\n",
      "Fold 3 - Loss: 0.7147095799446106, Accuracy: 0.6931094527244568\n",
      "Processing fold 4\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 5s 9ms/step - loss: 1.1308 - accuracy: 0.4776 - val_loss: 0.8744 - val_accuracy: 0.6022 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.9547 - accuracy: 0.5533 - val_loss: 0.8350 - val_accuracy: 0.6126 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.8747 - accuracy: 0.5960 - val_loss: 0.8192 - val_accuracy: 0.6294 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8278 - accuracy: 0.6206 - val_loss: 0.8096 - val_accuracy: 0.6259 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.8025 - accuracy: 0.6354 - val_loss: 0.8073 - val_accuracy: 0.6265 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7735 - accuracy: 0.6547 - val_loss: 0.8032 - val_accuracy: 0.6375 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7621 - accuracy: 0.6554 - val_loss: 0.7984 - val_accuracy: 0.6450 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 4ms/step - loss: 0.7472 - accuracy: 0.6639 - val_loss: 0.7900 - val_accuracy: 0.6491 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7370 - accuracy: 0.6722 - val_loss: 0.7909 - val_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7310 - accuracy: 0.6748 - val_loss: 0.7814 - val_accuracy: 0.6665 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7233 - accuracy: 0.6795 - val_loss: 0.7783 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.7178 - accuracy: 0.6824 - val_loss: 0.7781 - val_accuracy: 0.6682 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 2s 7ms/step - loss: 0.7028 - accuracy: 0.6936 - val_loss: 0.7820 - val_accuracy: 0.6665 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7039 - accuracy: 0.6950 - val_loss: 0.7736 - val_accuracy: 0.6746 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6884 - accuracy: 0.6995 - val_loss: 0.7721 - val_accuracy: 0.6734 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6890 - accuracy: 0.7053 - val_loss: 0.7668 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6740 - accuracy: 0.7055 - val_loss: 0.7685 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6681 - accuracy: 0.7134 - val_loss: 0.7620 - val_accuracy: 0.6821 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6703 - accuracy: 0.7085 - val_loss: 0.7656 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6680 - accuracy: 0.7057 - val_loss: 0.7649 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7649 - accuracy: 0.6856\n",
      "Fold 4 - Loss: 0.7649400234222412, Accuracy: 0.6855819225311279\n",
      "Processing fold 5\n",
      "Epoch 1/20\n",
      "216/216 [==============================] - 4s 7ms/step - loss: 1.1414 - accuracy: 0.4658 - val_loss: 0.8777 - val_accuracy: 0.5918 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.9457 - accuracy: 0.5539 - val_loss: 0.8255 - val_accuracy: 0.6242 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.8822 - accuracy: 0.5865 - val_loss: 0.8052 - val_accuracy: 0.6410 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.8312 - accuracy: 0.6159 - val_loss: 0.7977 - val_accuracy: 0.6393 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7987 - accuracy: 0.6364 - val_loss: 0.7904 - val_accuracy: 0.6462 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7869 - accuracy: 0.6418 - val_loss: 0.7814 - val_accuracy: 0.6653 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7689 - accuracy: 0.6520 - val_loss: 0.7791 - val_accuracy: 0.6584 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.7532 - accuracy: 0.6610 - val_loss: 0.7770 - val_accuracy: 0.6688 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "216/216 [==============================] - 1s 7ms/step - loss: 0.7423 - accuracy: 0.6641 - val_loss: 0.7716 - val_accuracy: 0.6734 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7302 - accuracy: 0.6701 - val_loss: 0.7650 - val_accuracy: 0.6769 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.7161 - accuracy: 0.6823 - val_loss: 0.7645 - val_accuracy: 0.6804 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.7108 - accuracy: 0.6845 - val_loss: 0.7591 - val_accuracy: 0.6850 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.7140 - accuracy: 0.6839 - val_loss: 0.7453 - val_accuracy: 0.6908 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6932 - accuracy: 0.6979 - val_loss: 0.7519 - val_accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6917 - accuracy: 0.6971 - val_loss: 0.7466 - val_accuracy: 0.6902 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "216/216 [==============================] - 1s 5ms/step - loss: 0.6893 - accuracy: 0.6982 - val_loss: 0.7436 - val_accuracy: 0.6937 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "216/216 [==============================] - 2s 8ms/step - loss: 0.6704 - accuracy: 0.7069 - val_loss: 0.7444 - val_accuracy: 0.6925 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "216/216 [==============================] - 2s 7ms/step - loss: 0.6633 - accuracy: 0.7076 - val_loss: 0.7391 - val_accuracy: 0.6925 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6735 - accuracy: 0.7023 - val_loss: 0.7358 - val_accuracy: 0.6920 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "216/216 [==============================] - 1s 6ms/step - loss: 0.6581 - accuracy: 0.7115 - val_loss: 0.7404 - val_accuracy: 0.6920 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7404 - accuracy: 0.6920\n",
      "Fold 5 - Loss: 0.7404054999351501, Accuracy: 0.691951334476471\n",
      "Average Accuracy across all folds: 0.6936081171035766\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Adjust n_splits as needed\n",
    "\n",
    "fold_accuracies = []  # List to store accuracy of each fold\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=100, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(SimpleRNN(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming Severity has 3 classes: 0, 1, 2\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_rnn, y_train, epochs=20, batch_size=32, validation_data=(X_val_rnn, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 9ms/step - loss: 0.9700 - accuracy: 0.5414 - val_loss: 0.8474 - val_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7774 - accuracy: 0.6466 - val_loss: 0.8116 - val_accuracy: 0.6343 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7170 - accuracy: 0.6754 - val_loss: 0.7947 - val_accuracy: 0.6308 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.6986 - val_loss: 0.7806 - val_accuracy: 0.6568 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6506 - accuracy: 0.7167 - val_loss: 0.7705 - val_accuracy: 0.6782 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6228 - accuracy: 0.7332 - val_loss: 0.7520 - val_accuracy: 0.6887 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6025 - accuracy: 0.7429 - val_loss: 0.7582 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5844 - accuracy: 0.7501 - val_loss: 0.7566 - val_accuracy: 0.6788 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5671 - accuracy: 0.7609 - val_loss: 0.7592 - val_accuracy: 0.6939 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5396 - accuracy: 0.7723 - val_loss: 0.7471 - val_accuracy: 0.6950 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5310 - accuracy: 0.7765 - val_loss: 0.7412 - val_accuracy: 0.6985 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5238 - accuracy: 0.7782 - val_loss: 0.7439 - val_accuracy: 0.7002 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5196 - accuracy: 0.7827 - val_loss: 0.7486 - val_accuracy: 0.7014 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5129 - accuracy: 0.7831 - val_loss: 0.7467 - val_accuracy: 0.6973 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5018 - accuracy: 0.7865 - val_loss: 0.7468 - val_accuracy: 0.7025 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4985 - accuracy: 0.7882 - val_loss: 0.7483 - val_accuracy: 0.7043 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.7412 - accuracy: 0.6985\n",
      "Fold 1 - Loss: 0.7412117123603821, Accuracy: 0.6984953880310059\n",
      "Processing fold 2\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 7ms/step - loss: 0.9720 - accuracy: 0.5321 - val_loss: 0.8314 - val_accuracy: 0.6109 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7907 - accuracy: 0.6341 - val_loss: 0.7719 - val_accuracy: 0.6624 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7279 - accuracy: 0.6706 - val_loss: 0.7591 - val_accuracy: 0.6659 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.6916 - val_loss: 0.7429 - val_accuracy: 0.6804 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6624 - accuracy: 0.7076 - val_loss: 0.7211 - val_accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6370 - accuracy: 0.7270 - val_loss: 0.7114 - val_accuracy: 0.7024 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6158 - accuracy: 0.7363 - val_loss: 0.7025 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5948 - accuracy: 0.7454 - val_loss: 0.6957 - val_accuracy: 0.7174 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5790 - accuracy: 0.7550 - val_loss: 0.6926 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5637 - accuracy: 0.7580 - val_loss: 0.6987 - val_accuracy: 0.7157 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5534 - accuracy: 0.7641 - val_loss: 0.6892 - val_accuracy: 0.7041 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.7668 - val_loss: 0.6881 - val_accuracy: 0.7226 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5338 - accuracy: 0.7704 - val_loss: 0.6845 - val_accuracy: 0.7215 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5240 - accuracy: 0.7687 - val_loss: 0.6878 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5175 - accuracy: 0.7741 - val_loss: 0.6837 - val_accuracy: 0.7192 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5122 - accuracy: 0.7723 - val_loss: 0.6853 - val_accuracy: 0.7290 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5054 - accuracy: 0.7759 - val_loss: 0.6884 - val_accuracy: 0.7250 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5003 - accuracy: 0.7791 - val_loss: 0.6833 - val_accuracy: 0.7348 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4959 - accuracy: 0.7810 - val_loss: 0.6988 - val_accuracy: 0.7273 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4920 - accuracy: 0.7812 - val_loss: 0.6901 - val_accuracy: 0.7342 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.6901 - accuracy: 0.7342\n",
      "Fold 2 - Loss: 0.6901327967643738, Accuracy: 0.7342212200164795\n",
      "Processing fold 3\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 7ms/step - loss: 0.9681 - accuracy: 0.5454 - val_loss: 0.8460 - val_accuracy: 0.5981 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7723 - accuracy: 0.6523 - val_loss: 0.7974 - val_accuracy: 0.6398 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7153 - accuracy: 0.6730 - val_loss: 0.7764 - val_accuracy: 0.6589 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6804 - accuracy: 0.7001 - val_loss: 0.7760 - val_accuracy: 0.6479 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6542 - accuracy: 0.7150 - val_loss: 0.7589 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.7263 - val_loss: 0.7564 - val_accuracy: 0.6862 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6120 - accuracy: 0.7360 - val_loss: 0.7421 - val_accuracy: 0.6983 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5946 - accuracy: 0.7435 - val_loss: 0.7372 - val_accuracy: 0.6937 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5770 - accuracy: 0.7586 - val_loss: 0.7272 - val_accuracy: 0.7064 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5638 - accuracy: 0.7547 - val_loss: 0.7368 - val_accuracy: 0.6960 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5539 - accuracy: 0.7619 - val_loss: 0.7343 - val_accuracy: 0.7064 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5418 - accuracy: 0.7702 - val_loss: 0.7361 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5193 - accuracy: 0.7780 - val_loss: 0.7239 - val_accuracy: 0.7215 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5129 - accuracy: 0.7797 - val_loss: 0.7255 - val_accuracy: 0.7093 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5080 - accuracy: 0.7816 - val_loss: 0.7218 - val_accuracy: 0.7058 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5040 - accuracy: 0.7833 - val_loss: 0.7290 - val_accuracy: 0.7151 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5013 - accuracy: 0.7819 - val_loss: 0.7294 - val_accuracy: 0.7122 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4980 - accuracy: 0.7809 - val_loss: 0.7251 - val_accuracy: 0.7151 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4865 - accuracy: 0.7869 - val_loss: 0.7256 - val_accuracy: 0.7087 - lr: 2.5000e-04\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.4843 - accuracy: 0.7890 - val_loss: 0.7258 - val_accuracy: 0.7140 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.7218 - accuracy: 0.7058\n",
      "Fold 3 - Loss: 0.7218441963195801, Accuracy: 0.7058482766151428\n",
      "Processing fold 4\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 3s 12ms/step - loss: 1.0065 - accuracy: 0.5273 - val_loss: 0.8707 - val_accuracy: 0.5854 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.7797 - accuracy: 0.6470 - val_loss: 0.8213 - val_accuracy: 0.6346 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7176 - accuracy: 0.6778 - val_loss: 0.8031 - val_accuracy: 0.6555 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6771 - accuracy: 0.7062 - val_loss: 0.7995 - val_accuracy: 0.6572 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.7180 - val_loss: 0.7947 - val_accuracy: 0.6618 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.6258 - accuracy: 0.7332 - val_loss: 0.7994 - val_accuracy: 0.6601 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.6041 - accuracy: 0.7422 - val_loss: 0.7898 - val_accuracy: 0.6763 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5867 - accuracy: 0.7484 - val_loss: 0.7853 - val_accuracy: 0.6908 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5690 - accuracy: 0.7580 - val_loss: 0.7956 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5550 - accuracy: 0.7581 - val_loss: 0.7798 - val_accuracy: 0.6983 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5459 - accuracy: 0.7660 - val_loss: 0.7880 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5381 - accuracy: 0.7693 - val_loss: 0.7898 - val_accuracy: 0.6960 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5281 - accuracy: 0.7736 - val_loss: 0.7986 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5048 - accuracy: 0.7813 - val_loss: 0.7911 - val_accuracy: 0.6925 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4987 - accuracy: 0.7852 - val_loss: 0.7919 - val_accuracy: 0.6914 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7798 - accuracy: 0.6983\n",
      "Fold 4 - Loss: 0.7798231244087219, Accuracy: 0.6983208060264587\n",
      "Processing fold 5\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 9ms/step - loss: 0.9728 - accuracy: 0.5464 - val_loss: 0.8536 - val_accuracy: 0.6196 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7746 - accuracy: 0.6393 - val_loss: 0.8050 - val_accuracy: 0.6416 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7121 - accuracy: 0.6804 - val_loss: 0.7828 - val_accuracy: 0.6642 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6786 - accuracy: 0.7018 - val_loss: 0.7764 - val_accuracy: 0.6636 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6520 - accuracy: 0.7118 - val_loss: 0.7723 - val_accuracy: 0.6711 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6236 - accuracy: 0.7243 - val_loss: 0.7607 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6007 - accuracy: 0.7411 - val_loss: 0.7516 - val_accuracy: 0.6966 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5833 - accuracy: 0.7458 - val_loss: 0.7526 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5679 - accuracy: 0.7499 - val_loss: 0.7554 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5553 - accuracy: 0.7555 - val_loss: 0.7571 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5303 - accuracy: 0.7700 - val_loss: 0.7487 - val_accuracy: 0.7070 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5216 - accuracy: 0.7746 - val_loss: 0.7521 - val_accuracy: 0.7076 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5168 - accuracy: 0.7778 - val_loss: 0.7504 - val_accuracy: 0.7169 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5119 - accuracy: 0.7777 - val_loss: 0.7542 - val_accuracy: 0.7099 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5000 - accuracy: 0.7854 - val_loss: 0.7510 - val_accuracy: 0.7099 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4970 - accuracy: 0.7845 - val_loss: 0.7512 - val_accuracy: 0.7151 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.7487 - accuracy: 0.7070\n",
      "Fold 5 - Loss: 0.7486546039581299, Accuracy: 0.7070063948631287\n",
      "Average Accuracy across all folds: 0.7087784171104431\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # Adjust n_splits as needed\n",
    "\n",
    "fold_accuracies = []  # List to store accuracy of each fold\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=50, return_sequences=False, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming Severity has 3 classes: 0, 1, 2\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_rnn, y_train, epochs=20, batch_size=64, validation_data=(X_val_rnn, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 8ms/step - loss: 0.9079 - accuracy: 0.5701 - val_loss: 0.8312 - val_accuracy: 0.5978 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7705 - accuracy: 0.6558 - val_loss: 0.8075 - val_accuracy: 0.6435 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7222 - accuracy: 0.6851 - val_loss: 0.7824 - val_accuracy: 0.6568 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6860 - accuracy: 0.7030 - val_loss: 0.7819 - val_accuracy: 0.6568 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6578 - accuracy: 0.7203 - val_loss: 0.7665 - val_accuracy: 0.6730 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.7254 - val_loss: 0.7522 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6181 - accuracy: 0.7339 - val_loss: 0.7529 - val_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6013 - accuracy: 0.7394 - val_loss: 0.7459 - val_accuracy: 0.6800 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5857 - accuracy: 0.7487 - val_loss: 0.7590 - val_accuracy: 0.6806 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5744 - accuracy: 0.7539 - val_loss: 0.7380 - val_accuracy: 0.6933 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5646 - accuracy: 0.7591 - val_loss: 0.7370 - val_accuracy: 0.6910 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5533 - accuracy: 0.7659 - val_loss: 0.7450 - val_accuracy: 0.6898 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5454 - accuracy: 0.7675 - val_loss: 0.7424 - val_accuracy: 0.6921 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5383 - accuracy: 0.7717 - val_loss: 0.7396 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5182 - accuracy: 0.7788 - val_loss: 0.7399 - val_accuracy: 0.6944 - lr: 5.0000e-04\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5137 - accuracy: 0.7820 - val_loss: 0.7392 - val_accuracy: 0.6950 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 1s 3ms/step - loss: 0.7370 - accuracy: 0.6910\n",
      "Fold 1 - Loss: 0.7369709610939026, Accuracy: 0.6909722089767456\n",
      "Processing fold 2\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 10ms/step - loss: 0.9026 - accuracy: 0.5682 - val_loss: 0.8075 - val_accuracy: 0.6306 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7716 - accuracy: 0.6512 - val_loss: 0.7671 - val_accuracy: 0.6543 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7245 - accuracy: 0.6780 - val_loss: 0.7386 - val_accuracy: 0.6769 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6933 - accuracy: 0.6987 - val_loss: 0.7342 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6673 - accuracy: 0.7125 - val_loss: 0.7252 - val_accuracy: 0.6850 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6463 - accuracy: 0.7276 - val_loss: 0.7120 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6302 - accuracy: 0.7293 - val_loss: 0.7037 - val_accuracy: 0.7030 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6144 - accuracy: 0.7364 - val_loss: 0.6993 - val_accuracy: 0.7006 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6014 - accuracy: 0.7432 - val_loss: 0.7029 - val_accuracy: 0.7012 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5881 - accuracy: 0.7487 - val_loss: 0.6825 - val_accuracy: 0.7058 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5769 - accuracy: 0.7522 - val_loss: 0.6861 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5680 - accuracy: 0.7586 - val_loss: 0.6789 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5598 - accuracy: 0.7612 - val_loss: 0.6744 - val_accuracy: 0.7099 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5520 - accuracy: 0.7664 - val_loss: 0.6757 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7681 - val_loss: 0.6672 - val_accuracy: 0.7197 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5382 - accuracy: 0.7703 - val_loss: 0.6707 - val_accuracy: 0.7226 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5324 - accuracy: 0.7719 - val_loss: 0.6769 - val_accuracy: 0.7151 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5254 - accuracy: 0.7768 - val_loss: 0.6648 - val_accuracy: 0.7296 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5202 - accuracy: 0.7783 - val_loss: 0.6642 - val_accuracy: 0.7290 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5154 - accuracy: 0.7825 - val_loss: 0.6717 - val_accuracy: 0.7244 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5125 - accuracy: 0.7806 - val_loss: 0.6639 - val_accuracy: 0.7226 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5076 - accuracy: 0.7814 - val_loss: 0.6661 - val_accuracy: 0.7290 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5046 - accuracy: 0.7861 - val_loss: 0.6648 - val_accuracy: 0.7244 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4992 - accuracy: 0.7877 - val_loss: 0.6624 - val_accuracy: 0.7331 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4973 - accuracy: 0.7865 - val_loss: 0.6643 - val_accuracy: 0.7342 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4949 - accuracy: 0.7885 - val_loss: 0.6636 - val_accuracy: 0.7365 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4931 - accuracy: 0.7904 - val_loss: 0.6641 - val_accuracy: 0.7209 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4767 - accuracy: 0.7972 - val_loss: 0.6628 - val_accuracy: 0.7302 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4746 - accuracy: 0.7981 - val_loss: 0.6623 - val_accuracy: 0.7331 - lr: 5.0000e-04\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4732 - accuracy: 0.7968 - val_loss: 0.6626 - val_accuracy: 0.7354 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6626 - accuracy: 0.7354\n",
      "Fold 2 - Loss: 0.662571370601654, Accuracy: 0.7353792786598206\n",
      "Processing fold 3\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 14ms/step - loss: 0.9086 - accuracy: 0.5716 - val_loss: 0.8260 - val_accuracy: 0.6109 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7677 - accuracy: 0.6539 - val_loss: 0.7869 - val_accuracy: 0.6462 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7173 - accuracy: 0.6868 - val_loss: 0.7683 - val_accuracy: 0.6665 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.6832 - accuracy: 0.7043 - val_loss: 0.7506 - val_accuracy: 0.6804 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.6569 - accuracy: 0.7150 - val_loss: 0.7387 - val_accuracy: 0.6862 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.6354 - accuracy: 0.7251 - val_loss: 0.7392 - val_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6193 - accuracy: 0.7327 - val_loss: 0.7244 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.6022 - accuracy: 0.7409 - val_loss: 0.7246 - val_accuracy: 0.7053 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5875 - accuracy: 0.7499 - val_loss: 0.7167 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5770 - accuracy: 0.7535 - val_loss: 0.7186 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5653 - accuracy: 0.7615 - val_loss: 0.7182 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5566 - accuracy: 0.7600 - val_loss: 0.7114 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5472 - accuracy: 0.7678 - val_loss: 0.7100 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5420 - accuracy: 0.7687 - val_loss: 0.7063 - val_accuracy: 0.7197 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5342 - accuracy: 0.7686 - val_loss: 0.7053 - val_accuracy: 0.7215 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.5281 - accuracy: 0.7759 - val_loss: 0.7006 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5215 - accuracy: 0.7771 - val_loss: 0.7034 - val_accuracy: 0.7238 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5166 - accuracy: 0.7800 - val_loss: 0.7093 - val_accuracy: 0.7047 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5128 - accuracy: 0.7800 - val_loss: 0.7067 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4960 - accuracy: 0.7906 - val_loss: 0.7029 - val_accuracy: 0.7151 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4923 - accuracy: 0.7882 - val_loss: 0.7049 - val_accuracy: 0.7203 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.7076\n",
      "Fold 3 - Loss: 0.7006359696388245, Accuracy: 0.7075853943824768\n",
      "Processing fold 4\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 10ms/step - loss: 0.9124 - accuracy: 0.5626 - val_loss: 0.8228 - val_accuracy: 0.6219 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7649 - accuracy: 0.6551 - val_loss: 0.7907 - val_accuracy: 0.6491 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.7137 - accuracy: 0.6823 - val_loss: 0.7787 - val_accuracy: 0.6613 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6799 - accuracy: 0.7049 - val_loss: 0.7660 - val_accuracy: 0.6688 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6524 - accuracy: 0.7207 - val_loss: 0.7684 - val_accuracy: 0.6705 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6315 - accuracy: 0.7286 - val_loss: 0.7518 - val_accuracy: 0.6746 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6129 - accuracy: 0.7402 - val_loss: 0.7502 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5973 - accuracy: 0.7435 - val_loss: 0.7593 - val_accuracy: 0.6734 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5862 - accuracy: 0.7502 - val_loss: 0.7492 - val_accuracy: 0.6891 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5715 - accuracy: 0.7607 - val_loss: 0.7377 - val_accuracy: 0.6902 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5614 - accuracy: 0.7592 - val_loss: 0.7433 - val_accuracy: 0.6902 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5532 - accuracy: 0.7616 - val_loss: 0.7365 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5444 - accuracy: 0.7715 - val_loss: 0.7367 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5362 - accuracy: 0.7723 - val_loss: 0.7335 - val_accuracy: 0.6948 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5311 - accuracy: 0.7725 - val_loss: 0.7402 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5222 - accuracy: 0.7784 - val_loss: 0.7434 - val_accuracy: 0.6954 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5189 - accuracy: 0.7765 - val_loss: 0.7379 - val_accuracy: 0.6937 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4998 - accuracy: 0.7880 - val_loss: 0.7332 - val_accuracy: 0.6983 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4959 - accuracy: 0.7881 - val_loss: 0.7366 - val_accuracy: 0.6995 - lr: 5.0000e-04\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4937 - accuracy: 0.7878 - val_loss: 0.7358 - val_accuracy: 0.7035 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4914 - accuracy: 0.7894 - val_loss: 0.7344 - val_accuracy: 0.7030 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4817 - accuracy: 0.7943 - val_loss: 0.7359 - val_accuracy: 0.7018 - lr: 2.5000e-04\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4805 - accuracy: 0.7929 - val_loss: 0.7368 - val_accuracy: 0.6966 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7332 - accuracy: 0.6983\n",
      "Fold 4 - Loss: 0.733208417892456, Accuracy: 0.6983208060264587\n",
      "Processing fold 5\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 12ms/step - loss: 0.9039 - accuracy: 0.5730 - val_loss: 0.8382 - val_accuracy: 0.6213 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7614 - accuracy: 0.6590 - val_loss: 0.7976 - val_accuracy: 0.6456 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7136 - accuracy: 0.6861 - val_loss: 0.7792 - val_accuracy: 0.6728 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6787 - accuracy: 0.7065 - val_loss: 0.7671 - val_accuracy: 0.6850 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6549 - accuracy: 0.7139 - val_loss: 0.7508 - val_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6348 - accuracy: 0.7234 - val_loss: 0.7594 - val_accuracy: 0.6804 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6168 - accuracy: 0.7338 - val_loss: 0.7345 - val_accuracy: 0.6995 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6004 - accuracy: 0.7419 - val_loss: 0.7398 - val_accuracy: 0.6972 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5890 - accuracy: 0.7487 - val_loss: 0.7360 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5782 - accuracy: 0.7493 - val_loss: 0.7309 - val_accuracy: 0.6937 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5662 - accuracy: 0.7534 - val_loss: 0.7270 - val_accuracy: 0.7041 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5564 - accuracy: 0.7592 - val_loss: 0.7287 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5477 - accuracy: 0.7625 - val_loss: 0.7244 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5413 - accuracy: 0.7618 - val_loss: 0.7194 - val_accuracy: 0.7099 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5334 - accuracy: 0.7661 - val_loss: 0.7274 - val_accuracy: 0.7053 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5276 - accuracy: 0.7700 - val_loss: 0.7172 - val_accuracy: 0.7099 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5193 - accuracy: 0.7772 - val_loss: 0.7259 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5162 - accuracy: 0.7745 - val_loss: 0.7218 - val_accuracy: 0.7174 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5113 - accuracy: 0.7813 - val_loss: 0.7260 - val_accuracy: 0.7111 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4919 - accuracy: 0.7882 - val_loss: 0.7236 - val_accuracy: 0.7140 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.4902 - accuracy: 0.7911 - val_loss: 0.7241 - val_accuracy: 0.7163 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7172 - accuracy: 0.7099\n",
      "Fold 5 - Loss: 0.7171620726585388, Accuracy: 0.7099015712738037\n",
      "Average Accuracy across all folds: 0.708431851863861\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in kf.split(X_scaled):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=70, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n",
    "    model.add(SimpleRNN(units=30, return_sequences=False))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = RMSprop(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_rnn, y_train, epochs=30, batch_size=64, validation_data=(X_val_rnn, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 9ms/step - loss: 1.8378 - accuracy: 0.4813 - val_loss: 1.4231 - val_accuracy: 0.5862 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.3290 - accuracy: 0.5906 - val_loss: 1.2087 - val_accuracy: 0.6209 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.1495 - accuracy: 0.6223 - val_loss: 1.0946 - val_accuracy: 0.6221 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0477 - accuracy: 0.6325 - val_loss: 1.0388 - val_accuracy: 0.6047 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9824 - accuracy: 0.6336 - val_loss: 0.9827 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9463 - accuracy: 0.6327 - val_loss: 0.9597 - val_accuracy: 0.6181 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.9188 - accuracy: 0.6400 - val_loss: 0.9372 - val_accuracy: 0.6209 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8955 - accuracy: 0.6404 - val_loss: 0.9219 - val_accuracy: 0.6221 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8897 - accuracy: 0.6395 - val_loss: 0.9162 - val_accuracy: 0.6163 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8788 - accuracy: 0.6345 - val_loss: 0.9140 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.8719 - accuracy: 0.6382 - val_loss: 0.9072 - val_accuracy: 0.6221 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8657 - accuracy: 0.6427 - val_loss: 0.9019 - val_accuracy: 0.6273 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8595 - accuracy: 0.6432 - val_loss: 0.9044 - val_accuracy: 0.6100 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8596 - accuracy: 0.6414 - val_loss: 0.8949 - val_accuracy: 0.6204 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8546 - accuracy: 0.6403 - val_loss: 0.8998 - val_accuracy: 0.6250 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8544 - accuracy: 0.6448 - val_loss: 0.8925 - val_accuracy: 0.6146 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8515 - accuracy: 0.6433 - val_loss: 0.8917 - val_accuracy: 0.6244 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8545 - accuracy: 0.6453 - val_loss: 0.8900 - val_accuracy: 0.6308 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8529 - accuracy: 0.6401 - val_loss: 0.8839 - val_accuracy: 0.6354 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8521 - accuracy: 0.6446 - val_loss: 0.8866 - val_accuracy: 0.6267 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8459 - accuracy: 0.6527 - val_loss: 0.8875 - val_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8536 - accuracy: 0.6400 - val_loss: 0.8954 - val_accuracy: 0.6152 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8334 - accuracy: 0.6598 - val_loss: 0.8810 - val_accuracy: 0.6192 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8271 - accuracy: 0.6563 - val_loss: 0.8721 - val_accuracy: 0.6192 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8241 - accuracy: 0.6520 - val_loss: 0.8730 - val_accuracy: 0.6267 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8172 - accuracy: 0.6624 - val_loss: 0.8686 - val_accuracy: 0.6285 - lr: 5.0000e-04\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8211 - accuracy: 0.6605 - val_loss: 0.8671 - val_accuracy: 0.6244 - lr: 5.0000e-04\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8187 - accuracy: 0.6614 - val_loss: 0.8701 - val_accuracy: 0.6175 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8132 - accuracy: 0.6616 - val_loss: 0.8683 - val_accuracy: 0.6360 - lr: 5.0000e-04\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8160 - accuracy: 0.6526 - val_loss: 0.8676 - val_accuracy: 0.6279 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 1s 3ms/step - loss: 0.8676 - accuracy: 0.6279\n",
      "Fold 1 - Loss: 0.8675565123558044, Accuracy: 0.6278935074806213\n",
      "Processing fold 2\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 4s 12ms/step - loss: 1.8921 - accuracy: 0.4860 - val_loss: 1.4492 - val_accuracy: 0.6091 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.3887 - accuracy: 0.5816 - val_loss: 1.2387 - val_accuracy: 0.6242 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1960 - accuracy: 0.6154 - val_loss: 1.1260 - val_accuracy: 0.6317 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.0894 - accuracy: 0.6298 - val_loss: 1.0390 - val_accuracy: 0.6369 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 9ms/step - loss: 1.0249 - accuracy: 0.6277 - val_loss: 0.9993 - val_accuracy: 0.6236 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 9ms/step - loss: 0.9734 - accuracy: 0.6353 - val_loss: 0.9666 - val_accuracy: 0.6242 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.9460 - accuracy: 0.6302 - val_loss: 0.9366 - val_accuracy: 0.6381 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 9ms/step - loss: 0.9170 - accuracy: 0.6353 - val_loss: 0.9143 - val_accuracy: 0.6497 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9011 - accuracy: 0.6355 - val_loss: 0.8994 - val_accuracy: 0.6369 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8859 - accuracy: 0.6358 - val_loss: 0.9018 - val_accuracy: 0.6225 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 12ms/step - loss: 0.8789 - accuracy: 0.6367 - val_loss: 0.8885 - val_accuracy: 0.6283 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 9ms/step - loss: 0.8742 - accuracy: 0.6296 - val_loss: 0.8833 - val_accuracy: 0.6479 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8700 - accuracy: 0.6347 - val_loss: 0.8871 - val_accuracy: 0.6259 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8638 - accuracy: 0.6327 - val_loss: 0.8737 - val_accuracy: 0.6317 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8572 - accuracy: 0.6308 - val_loss: 0.8684 - val_accuracy: 0.6439 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8575 - accuracy: 0.6370 - val_loss: 0.8706 - val_accuracy: 0.6456 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8529 - accuracy: 0.6390 - val_loss: 0.8666 - val_accuracy: 0.6479 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8533 - accuracy: 0.6424 - val_loss: 0.8723 - val_accuracy: 0.6306 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8562 - accuracy: 0.6350 - val_loss: 0.8690 - val_accuracy: 0.6410 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.8483 - accuracy: 0.6403 - val_loss: 0.8697 - val_accuracy: 0.6410 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8347 - accuracy: 0.6499 - val_loss: 0.8579 - val_accuracy: 0.6393 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8256 - accuracy: 0.6596 - val_loss: 0.8594 - val_accuracy: 0.6479 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8203 - accuracy: 0.6547 - val_loss: 0.8526 - val_accuracy: 0.6508 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8258 - accuracy: 0.6528 - val_loss: 0.8539 - val_accuracy: 0.6410 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8178 - accuracy: 0.6512 - val_loss: 0.8553 - val_accuracy: 0.6329 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8246 - accuracy: 0.6461 - val_loss: 0.8496 - val_accuracy: 0.6450 - lr: 5.0000e-04\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8211 - accuracy: 0.6512 - val_loss: 0.8463 - val_accuracy: 0.6294 - lr: 5.0000e-04\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8199 - accuracy: 0.6502 - val_loss: 0.8494 - val_accuracy: 0.6532 - lr: 5.0000e-04\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8181 - accuracy: 0.6509 - val_loss: 0.8452 - val_accuracy: 0.6410 - lr: 5.0000e-04\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8156 - accuracy: 0.6513 - val_loss: 0.8455 - val_accuracy: 0.6601 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.8455 - accuracy: 0.6601\n",
      "Fold 2 - Loss: 0.8454775214195251, Accuracy: 0.6601042151451111\n",
      "Processing fold 3\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 4s 8ms/step - loss: 1.7593 - accuracy: 0.4918 - val_loss: 1.3725 - val_accuracy: 0.5871 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.2640 - accuracy: 0.5988 - val_loss: 1.1764 - val_accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.0993 - accuracy: 0.6263 - val_loss: 1.0735 - val_accuracy: 0.6242 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 1.0103 - accuracy: 0.6309 - val_loss: 1.0130 - val_accuracy: 0.6207 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9636 - accuracy: 0.6276 - val_loss: 0.9831 - val_accuracy: 0.6219 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9265 - accuracy: 0.6373 - val_loss: 0.9562 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9050 - accuracy: 0.6413 - val_loss: 0.9415 - val_accuracy: 0.6312 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8862 - accuracy: 0.6387 - val_loss: 0.9366 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8777 - accuracy: 0.6370 - val_loss: 0.9200 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8760 - accuracy: 0.6409 - val_loss: 0.9185 - val_accuracy: 0.5993 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8670 - accuracy: 0.6363 - val_loss: 0.9145 - val_accuracy: 0.6265 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8612 - accuracy: 0.6408 - val_loss: 0.9077 - val_accuracy: 0.6288 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8534 - accuracy: 0.6431 - val_loss: 0.9033 - val_accuracy: 0.6346 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8529 - accuracy: 0.6402 - val_loss: 0.9077 - val_accuracy: 0.6236 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8512 - accuracy: 0.6444 - val_loss: 0.9043 - val_accuracy: 0.6103 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8502 - accuracy: 0.6415 - val_loss: 0.9108 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8334 - accuracy: 0.6545 - val_loss: 0.8971 - val_accuracy: 0.6312 - lr: 5.0000e-04\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8249 - accuracy: 0.6555 - val_loss: 0.8945 - val_accuracy: 0.6225 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8280 - accuracy: 0.6445 - val_loss: 0.8906 - val_accuracy: 0.6294 - lr: 5.0000e-04\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8143 - accuracy: 0.6551 - val_loss: 0.8925 - val_accuracy: 0.6364 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8185 - accuracy: 0.6545 - val_loss: 0.8857 - val_accuracy: 0.6306 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8177 - accuracy: 0.6532 - val_loss: 0.8835 - val_accuracy: 0.6288 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8163 - accuracy: 0.6526 - val_loss: 0.8956 - val_accuracy: 0.6312 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8134 - accuracy: 0.6583 - val_loss: 0.8929 - val_accuracy: 0.6352 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8192 - accuracy: 0.6468 - val_loss: 0.8871 - val_accuracy: 0.6254 - lr: 5.0000e-04\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8024 - accuracy: 0.6651 - val_loss: 0.8790 - val_accuracy: 0.6398 - lr: 2.5000e-04\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7959 - accuracy: 0.6655 - val_loss: 0.8794 - val_accuracy: 0.6340 - lr: 2.5000e-04\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7986 - accuracy: 0.6685 - val_loss: 0.8789 - val_accuracy: 0.6358 - lr: 2.5000e-04\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7981 - accuracy: 0.6645 - val_loss: 0.8801 - val_accuracy: 0.6358 - lr: 2.5000e-04\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7929 - accuracy: 0.6613 - val_loss: 0.8785 - val_accuracy: 0.6358 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.8785 - accuracy: 0.6358\n",
      "Fold 3 - Loss: 0.8784746527671814, Accuracy: 0.6357846260070801\n",
      "Processing fold 4\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 3s 9ms/step - loss: 1.8214 - accuracy: 0.4798 - val_loss: 1.4139 - val_accuracy: 0.5970 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 1.3136 - accuracy: 0.5949 - val_loss: 1.2015 - val_accuracy: 0.6138 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1295 - accuracy: 0.6217 - val_loss: 1.1010 - val_accuracy: 0.6230 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.0376 - accuracy: 0.6376 - val_loss: 1.0309 - val_accuracy: 0.6259 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9770 - accuracy: 0.6337 - val_loss: 0.9951 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9426 - accuracy: 0.6325 - val_loss: 0.9651 - val_accuracy: 0.6167 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9082 - accuracy: 0.6424 - val_loss: 0.9468 - val_accuracy: 0.6190 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.9018 - accuracy: 0.6399 - val_loss: 0.9202 - val_accuracy: 0.6149 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8852 - accuracy: 0.6399 - val_loss: 0.9239 - val_accuracy: 0.6190 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8724 - accuracy: 0.6452 - val_loss: 0.9133 - val_accuracy: 0.6225 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8675 - accuracy: 0.6418 - val_loss: 0.9070 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8648 - accuracy: 0.6421 - val_loss: 0.9040 - val_accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8538 - accuracy: 0.6454 - val_loss: 0.9079 - val_accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 10ms/step - loss: 0.8568 - accuracy: 0.6397 - val_loss: 0.9012 - val_accuracy: 0.6207 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8504 - accuracy: 0.6464 - val_loss: 0.9011 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8515 - accuracy: 0.6492 - val_loss: 0.8943 - val_accuracy: 0.6207 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8486 - accuracy: 0.6445 - val_loss: 0.9036 - val_accuracy: 0.6091 - lr: 0.0010\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8465 - accuracy: 0.6470 - val_loss: 0.8912 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8466 - accuracy: 0.6464 - val_loss: 0.8887 - val_accuracy: 0.6254 - lr: 0.0010\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8455 - accuracy: 0.6437 - val_loss: 0.8914 - val_accuracy: 0.6340 - lr: 0.0010\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8475 - accuracy: 0.6481 - val_loss: 0.8945 - val_accuracy: 0.6109 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8501 - accuracy: 0.6452 - val_loss: 0.8879 - val_accuracy: 0.6277 - lr: 0.0010\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8438 - accuracy: 0.6502 - val_loss: 0.8850 - val_accuracy: 0.6254 - lr: 0.0010\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8429 - accuracy: 0.6458 - val_loss: 0.8825 - val_accuracy: 0.6259 - lr: 0.0010\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8476 - accuracy: 0.6467 - val_loss: 0.8944 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8455 - accuracy: 0.6477 - val_loss: 0.8985 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8407 - accuracy: 0.6539 - val_loss: 0.8807 - val_accuracy: 0.6369 - lr: 0.0010\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8436 - accuracy: 0.6499 - val_loss: 0.8796 - val_accuracy: 0.6259 - lr: 0.0010\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8401 - accuracy: 0.6494 - val_loss: 0.8831 - val_accuracy: 0.6323 - lr: 0.0010\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8442 - accuracy: 0.6480 - val_loss: 0.8880 - val_accuracy: 0.6340 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.8880 - accuracy: 0.6340\n",
      "Fold 4 - Loss: 0.888004720211029, Accuracy: 0.6340475082397461\n",
      "Processing fold 5\n",
      "Epoch 1/30\n",
      "108/108 [==============================] - 4s 10ms/step - loss: 1.8178 - accuracy: 0.4915 - val_loss: 1.3941 - val_accuracy: 0.6051 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 1.3152 - accuracy: 0.5983 - val_loss: 1.1856 - val_accuracy: 0.6254 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 1.1466 - accuracy: 0.6175 - val_loss: 1.0875 - val_accuracy: 0.6439 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 1.0461 - accuracy: 0.6290 - val_loss: 1.0236 - val_accuracy: 0.6236 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9945 - accuracy: 0.6238 - val_loss: 0.9790 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.9453 - accuracy: 0.6354 - val_loss: 0.9439 - val_accuracy: 0.6352 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9198 - accuracy: 0.6399 - val_loss: 0.9267 - val_accuracy: 0.6514 - lr: 0.0010\n",
      "Epoch 8/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.9032 - accuracy: 0.6405 - val_loss: 0.9243 - val_accuracy: 0.6375 - lr: 0.0010\n",
      "Epoch 9/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8943 - accuracy: 0.6399 - val_loss: 0.9074 - val_accuracy: 0.6213 - lr: 0.0010\n",
      "Epoch 10/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8861 - accuracy: 0.6321 - val_loss: 0.9039 - val_accuracy: 0.6230 - lr: 0.0010\n",
      "Epoch 11/30\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.8728 - accuracy: 0.6369 - val_loss: 0.8932 - val_accuracy: 0.6271 - lr: 0.0010\n",
      "Epoch 12/30\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.8701 - accuracy: 0.6380 - val_loss: 0.8879 - val_accuracy: 0.6358 - lr: 0.0010\n",
      "Epoch 13/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8646 - accuracy: 0.6338 - val_loss: 0.8812 - val_accuracy: 0.6300 - lr: 0.0010\n",
      "Epoch 14/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8632 - accuracy: 0.6324 - val_loss: 0.8822 - val_accuracy: 0.6236 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8591 - accuracy: 0.6395 - val_loss: 0.8892 - val_accuracy: 0.6091 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8600 - accuracy: 0.6358 - val_loss: 0.8865 - val_accuracy: 0.6335 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.8364 - accuracy: 0.6480 - val_loss: 0.8696 - val_accuracy: 0.6410 - lr: 5.0000e-04\n",
      "Epoch 18/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8377 - accuracy: 0.6442 - val_loss: 0.8689 - val_accuracy: 0.6433 - lr: 5.0000e-04\n",
      "Epoch 19/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8309 - accuracy: 0.6545 - val_loss: 0.8670 - val_accuracy: 0.6450 - lr: 5.0000e-04\n",
      "Epoch 20/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8289 - accuracy: 0.6515 - val_loss: 0.8618 - val_accuracy: 0.6578 - lr: 5.0000e-04\n",
      "Epoch 21/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8278 - accuracy: 0.6496 - val_loss: 0.8580 - val_accuracy: 0.6450 - lr: 5.0000e-04\n",
      "Epoch 22/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8271 - accuracy: 0.6490 - val_loss: 0.8706 - val_accuracy: 0.6236 - lr: 5.0000e-04\n",
      "Epoch 23/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8244 - accuracy: 0.6497 - val_loss: 0.8605 - val_accuracy: 0.6462 - lr: 5.0000e-04\n",
      "Epoch 24/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8251 - accuracy: 0.6516 - val_loss: 0.8623 - val_accuracy: 0.6358 - lr: 5.0000e-04\n",
      "Epoch 25/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8167 - accuracy: 0.6545 - val_loss: 0.8584 - val_accuracy: 0.6462 - lr: 2.5000e-04\n",
      "Epoch 26/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8162 - accuracy: 0.6564 - val_loss: 0.8560 - val_accuracy: 0.6479 - lr: 2.5000e-04\n",
      "Epoch 27/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8073 - accuracy: 0.6609 - val_loss: 0.8576 - val_accuracy: 0.6393 - lr: 2.5000e-04\n",
      "Epoch 28/30\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.8063 - accuracy: 0.6612 - val_loss: 0.8563 - val_accuracy: 0.6381 - lr: 2.5000e-04\n",
      "Epoch 29/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.8086 - accuracy: 0.6574 - val_loss: 0.8562 - val_accuracy: 0.6508 - lr: 2.5000e-04\n",
      "Epoch 30/30\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7990 - accuracy: 0.6674 - val_loss: 0.8528 - val_accuracy: 0.6520 - lr: 1.2500e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.8528 - accuracy: 0.6520\n",
      "Fold 5 - Loss: 0.8528148531913757, Accuracy: 0.6519976854324341\n",
      "Average Accuracy across all folds: 0.6419655084609985\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in skf.split(X_scaled, y_encoded):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=50, return_sequences=True, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(SimpleRNN(units=25, return_sequences=False, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_rnn, y_train, epochs=30, batch_size=64, validation_data=(X_val_rnn, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 7ms/step - loss: 0.9853 - accuracy: 0.5401 - val_loss: 0.8610 - val_accuracy: 0.5943 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7810 - accuracy: 0.6479 - val_loss: 0.8090 - val_accuracy: 0.6296 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7195 - accuracy: 0.6796 - val_loss: 0.7909 - val_accuracy: 0.6487 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.6966 - val_loss: 0.7786 - val_accuracy: 0.6464 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.7167 - val_loss: 0.7776 - val_accuracy: 0.6690 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6309 - accuracy: 0.7332 - val_loss: 0.7619 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6104 - accuracy: 0.7417 - val_loss: 0.7530 - val_accuracy: 0.6834 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5901 - accuracy: 0.7493 - val_loss: 0.7508 - val_accuracy: 0.6777 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5730 - accuracy: 0.7588 - val_loss: 0.7434 - val_accuracy: 0.6869 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5584 - accuracy: 0.7606 - val_loss: 0.7335 - val_accuracy: 0.6979 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5475 - accuracy: 0.7682 - val_loss: 0.7322 - val_accuracy: 0.6973 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5402 - accuracy: 0.7687 - val_loss: 0.7356 - val_accuracy: 0.7014 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5295 - accuracy: 0.7732 - val_loss: 0.7372 - val_accuracy: 0.6968 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5185 - accuracy: 0.7781 - val_loss: 0.7352 - val_accuracy: 0.7124 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5007 - accuracy: 0.7816 - val_loss: 0.7262 - val_accuracy: 0.7072 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4934 - accuracy: 0.7892 - val_loss: 0.7250 - val_accuracy: 0.7130 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4913 - accuracy: 0.7840 - val_loss: 0.7244 - val_accuracy: 0.7089 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4877 - accuracy: 0.7876 - val_loss: 0.7286 - val_accuracy: 0.7031 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4848 - accuracy: 0.7904 - val_loss: 0.7268 - val_accuracy: 0.7025 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.7885 - val_loss: 0.7330 - val_accuracy: 0.7054 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7330 - accuracy: 0.7054\n",
      "Fold 1 - Loss: 0.7330005168914795, Accuracy: 0.7054398059844971\n",
      "Processing fold 2\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 8ms/step - loss: 0.9784 - accuracy: 0.5465 - val_loss: 0.8459 - val_accuracy: 0.6028 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7735 - accuracy: 0.6455 - val_loss: 0.7977 - val_accuracy: 0.6312 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7126 - accuracy: 0.6817 - val_loss: 0.7714 - val_accuracy: 0.6595 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6759 - accuracy: 0.6994 - val_loss: 0.7699 - val_accuracy: 0.6572 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6486 - accuracy: 0.7199 - val_loss: 0.7520 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6219 - accuracy: 0.7301 - val_loss: 0.7350 - val_accuracy: 0.6931 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6003 - accuracy: 0.7383 - val_loss: 0.7330 - val_accuracy: 0.6966 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5831 - accuracy: 0.7480 - val_loss: 0.7199 - val_accuracy: 0.7041 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 8ms/step - loss: 0.5656 - accuracy: 0.7577 - val_loss: 0.7204 - val_accuracy: 0.7030 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5555 - accuracy: 0.7599 - val_loss: 0.7232 - val_accuracy: 0.7053 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5430 - accuracy: 0.7662 - val_loss: 0.7111 - val_accuracy: 0.7058 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5318 - accuracy: 0.7712 - val_loss: 0.7212 - val_accuracy: 0.7070 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5258 - accuracy: 0.7687 - val_loss: 0.7211 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5168 - accuracy: 0.7762 - val_loss: 0.7100 - val_accuracy: 0.7186 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5114 - accuracy: 0.7752 - val_loss: 0.7134 - val_accuracy: 0.7093 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5052 - accuracy: 0.7778 - val_loss: 0.7193 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4998 - accuracy: 0.7784 - val_loss: 0.7197 - val_accuracy: 0.7197 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4817 - accuracy: 0.7903 - val_loss: 0.7099 - val_accuracy: 0.7221 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.7911 - val_loss: 0.7146 - val_accuracy: 0.7197 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4734 - accuracy: 0.7898 - val_loss: 0.7176 - val_accuracy: 0.7128 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7176 - accuracy: 0.7128\n",
      "Fold 2 - Loss: 0.7175613641738892, Accuracy: 0.7127967476844788\n",
      "Processing fold 3\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 8ms/step - loss: 0.9768 - accuracy: 0.5423 - val_loss: 0.8675 - val_accuracy: 0.5987 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.7722 - accuracy: 0.6529 - val_loss: 0.8311 - val_accuracy: 0.6184 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7122 - accuracy: 0.6778 - val_loss: 0.7982 - val_accuracy: 0.6543 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6726 - accuracy: 0.7073 - val_loss: 0.7978 - val_accuracy: 0.6601 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6450 - accuracy: 0.7140 - val_loss: 0.7899 - val_accuracy: 0.6642 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6171 - accuracy: 0.7322 - val_loss: 0.7776 - val_accuracy: 0.6775 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5986 - accuracy: 0.7405 - val_loss: 0.7838 - val_accuracy: 0.6763 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5834 - accuracy: 0.7484 - val_loss: 0.7745 - val_accuracy: 0.6862 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5660 - accuracy: 0.7565 - val_loss: 0.7700 - val_accuracy: 0.6885 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5537 - accuracy: 0.7580 - val_loss: 0.7708 - val_accuracy: 0.6914 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5416 - accuracy: 0.7691 - val_loss: 0.7628 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5299 - accuracy: 0.7699 - val_loss: 0.7667 - val_accuracy: 0.6960 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5252 - accuracy: 0.7720 - val_loss: 0.7557 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5172 - accuracy: 0.7722 - val_loss: 0.7668 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5120 - accuracy: 0.7768 - val_loss: 0.7606 - val_accuracy: 0.6977 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.5045 - accuracy: 0.7801 - val_loss: 0.7568 - val_accuracy: 0.7001 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 0s 4ms/step - loss: 0.4863 - accuracy: 0.7875 - val_loss: 0.7541 - val_accuracy: 0.7058 - lr: 5.0000e-04\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4819 - accuracy: 0.7914 - val_loss: 0.7577 - val_accuracy: 0.7047 - lr: 5.0000e-04\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4786 - accuracy: 0.7933 - val_loss: 0.7575 - val_accuracy: 0.7035 - lr: 5.0000e-04\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4770 - accuracy: 0.7927 - val_loss: 0.7605 - val_accuracy: 0.7093 - lr: 5.0000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7605 - accuracy: 0.7093\n",
      "Fold 3 - Loss: 0.7604963779449463, Accuracy: 0.7093225121498108\n",
      "Processing fold 4\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 8ms/step - loss: 0.9889 - accuracy: 0.5347 - val_loss: 0.8641 - val_accuracy: 0.5929 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7817 - accuracy: 0.6455 - val_loss: 0.8043 - val_accuracy: 0.6427 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7176 - accuracy: 0.6743 - val_loss: 0.7752 - val_accuracy: 0.6561 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6801 - accuracy: 0.7013 - val_loss: 0.7621 - val_accuracy: 0.6728 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6543 - accuracy: 0.7120 - val_loss: 0.7522 - val_accuracy: 0.6752 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6273 - accuracy: 0.7280 - val_loss: 0.7433 - val_accuracy: 0.6873 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.6073 - accuracy: 0.7416 - val_loss: 0.7282 - val_accuracy: 0.7001 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5907 - accuracy: 0.7483 - val_loss: 0.7148 - val_accuracy: 0.6943 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5736 - accuracy: 0.7538 - val_loss: 0.7108 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5600 - accuracy: 0.7610 - val_loss: 0.7070 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7636 - val_loss: 0.7030 - val_accuracy: 0.7082 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5380 - accuracy: 0.7725 - val_loss: 0.7037 - val_accuracy: 0.7111 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5286 - accuracy: 0.7706 - val_loss: 0.7116 - val_accuracy: 0.7047 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5185 - accuracy: 0.7748 - val_loss: 0.7027 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5136 - accuracy: 0.7749 - val_loss: 0.7027 - val_accuracy: 0.7105 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5058 - accuracy: 0.7810 - val_loss: 0.7043 - val_accuracy: 0.7128 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5018 - accuracy: 0.7830 - val_loss: 0.7023 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4969 - accuracy: 0.7803 - val_loss: 0.7163 - val_accuracy: 0.7169 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4924 - accuracy: 0.7843 - val_loss: 0.7007 - val_accuracy: 0.7209 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4892 - accuracy: 0.7825 - val_loss: 0.6983 - val_accuracy: 0.7145 - lr: 0.0010\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6983 - accuracy: 0.7145\n",
      "Fold 4 - Loss: 0.6982654333114624, Accuracy: 0.7145338654518127\n",
      "Processing fold 5\n",
      "Epoch 1/20\n",
      "108/108 [==============================] - 2s 9ms/step - loss: 0.9801 - accuracy: 0.5389 - val_loss: 0.8400 - val_accuracy: 0.6005 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.7829 - accuracy: 0.6418 - val_loss: 0.7904 - val_accuracy: 0.6433 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.7237 - accuracy: 0.6746 - val_loss: 0.7729 - val_accuracy: 0.6717 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6890 - accuracy: 0.6949 - val_loss: 0.7635 - val_accuracy: 0.6671 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.6605 - accuracy: 0.7099 - val_loss: 0.7513 - val_accuracy: 0.6838 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6331 - accuracy: 0.7254 - val_loss: 0.7323 - val_accuracy: 0.6995 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.7377 - val_loss: 0.7319 - val_accuracy: 0.7064 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5945 - accuracy: 0.7454 - val_loss: 0.7232 - val_accuracy: 0.6972 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5779 - accuracy: 0.7502 - val_loss: 0.7223 - val_accuracy: 0.7093 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5641 - accuracy: 0.7564 - val_loss: 0.7222 - val_accuracy: 0.6948 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.5525 - accuracy: 0.7610 - val_loss: 0.7268 - val_accuracy: 0.7116 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5397 - accuracy: 0.7642 - val_loss: 0.7227 - val_accuracy: 0.7134 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "108/108 [==============================] - 1s 7ms/step - loss: 0.5179 - accuracy: 0.7777 - val_loss: 0.7088 - val_accuracy: 0.7255 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.5108 - accuracy: 0.7830 - val_loss: 0.7094 - val_accuracy: 0.7238 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5065 - accuracy: 0.7803 - val_loss: 0.7121 - val_accuracy: 0.7232 - lr: 5.0000e-04\n",
      "Epoch 16/20\n",
      "108/108 [==============================] - 0s 5ms/step - loss: 0.5038 - accuracy: 0.7825 - val_loss: 0.7176 - val_accuracy: 0.7203 - lr: 5.0000e-04\n",
      "Epoch 17/20\n",
      "108/108 [==============================] - 1s 6ms/step - loss: 0.4928 - accuracy: 0.7890 - val_loss: 0.7136 - val_accuracy: 0.7226 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "108/108 [==============================] - 1s 5ms/step - loss: 0.4907 - accuracy: 0.7877 - val_loss: 0.7112 - val_accuracy: 0.7290 - lr: 2.5000e-04\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.7088 - accuracy: 0.7255\n",
      "Fold 5 - Loss: 0.7087526917457581, Accuracy: 0.7255356311798096\n",
      "Average Accuracy across all folds: 0.7135257124900818\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(r\"D:\\Github portfolio\\Hemophilia\\Hemophilia-Severity-Predictor\\datasets\\resampled_data_adasyn.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop(columns=['Severity'])\n",
    "y = df['Severity']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "# Stratified K-Fold Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "fold = 1\n",
    "for train_index, val_index in skf.split(X_scaled, y_encoded):\n",
    "    print(f\"Processing fold {fold}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    y_train, y_val = y_one_hot[train_index], y_one_hot[val_index]\n",
    "    \n",
    "    # Reshape for RNN\n",
    "    X_train_rnn = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_val_rnn = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "    \n",
    "    # Define the RNN model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(units=50, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2]), return_sequences=False))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_rnn, y_train, epochs=20, batch_size=64, validation_data=(X_val_rnn, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_val_rnn, y_val)\n",
    "    print(f'Fold {fold} - Loss: {loss}, Accuracy: {accuracy}')\n",
    "    \n",
    "    # Store the accuracy of this fold\n",
    "    fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Increment fold counter\n",
    "    fold += 1\n",
    "\n",
    "    # Clear the model from memory\n",
    "    K.clear_session()\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Average Accuracy across all folds: {average_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
